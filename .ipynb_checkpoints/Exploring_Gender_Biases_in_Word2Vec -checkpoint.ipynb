{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XP3YOprVwyX"
   },
   "source": [
    "# Exploring Gender Biases in Word2Vec \n",
    "\n",
    "This notebook processes the data from the Gutenburg and Wikipedia datasets. Then, it trains the words agains Genism's word2Vec model, a custom skip-gram based word2vec model and our debiasing solutions. \n",
    "\n",
    "To measure biases that are hidden within these embeddings created by the models, we explore the direct bias, indirect bias, and the WEAT metric for comparision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4LSSjBdfJhI",
    "outputId": "dfd47a58-6b57-4110-9160-f206ad21f54a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#library imports\n",
    "import string\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "import tempfile\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "#gutenberg data import\n",
    "from gutenberg_data import get_urls, read_data_from_urls\n",
    "\n",
    "#models import\n",
    "from gensim.models import Word2Vec\n",
    "from custom_word2vec import Custom_Word2Vec\n",
    "\n",
    "#plots\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhIMaI4vV8VI"
   },
   "source": [
    "## 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RzDntNYcmVec"
   },
   "outputs": [],
   "source": [
    "def create_tokens(data):\n",
    "    \"\"\"\n",
    "    creates tokenized list with puncuation and digits removed and lowercase words \n",
    "\n",
    "    :param data: list of strings\n",
    "    :return: a list of tokens \n",
    "    \"\"\" \n",
    "    tokens = []\n",
    "    for sentance in data:\n",
    "        sentance = sentance.translate(str.maketrans('', '', string.punctuation)) #remove punctuations\n",
    "        sentance = sentance.translate(str.maketrans('', '', string.digits)) #remove digits\n",
    "        tokenizer = get_tokenizer(\"basic_english\", language=\"en\") #remove unessasary characters, splits into spaces\n",
    "        tokens.append(tokenizer(sentance))\n",
    "  \n",
    "    return tokens\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTskJcpBpDEt"
   },
   "source": [
    "**Gutenburg Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsTQM81GpDEu",
    "outputId": "12fedd03-76dd-4c73-f874-c921b02c98fc"
   },
   "outputs": [],
   "source": [
    "# Gutenburg\n",
    "train_urls = get_urls('test')\n",
    "train_data_g = read_data_from_urls(train_urls)\n",
    "train_data_g=str(train_data_g).split('.')\n",
    "\n",
    "rough_guten = create_tokens(train_data_g)\n",
    "\n",
    "sentences_gutenburg = []\n",
    "for sentence in rough_guten:\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if 'rn' in word:\n",
    "            temp = word.replace('rn', ' ')\n",
    "            while temp and temp[0] == ' ':\n",
    "                temp = temp.replace(' ', '')\n",
    "            while temp and '  ' in temp:\n",
    "                temp = temp.replace('  ', '')\n",
    "            if ' ' in temp:\n",
    "                temp = temp.split(' ')\n",
    "                for word_thing in temp:\n",
    "                    new_sentence.append(word_thing)\n",
    "            else:\n",
    "                new_sentence.append(temp)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    sentences_gutenburg.append(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PcqA9wapDEv"
   },
   "source": [
    "**Wikipedia Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdY5fDuEpDEw",
    "outputId": "5c3c79b9-63a1-4f27-a540-fa70fc990d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "#Wikipedia\n",
    "f = open('wikipedia-en-1000.json')\n",
    "train_data_w = json.load(f)\n",
    "train_data_w =str(train_data_w[0]).split('.')\n",
    "\n",
    "sentences_wikipedia = create_tokens(train_data_w)\n",
    "print(len(sentences_wikipedia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGQ-ic0KpDEw"
   },
   "source": [
    "## 2 - Genism Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vADvrY8rDeuq"
   },
   "source": [
    "Using: Skip-gram training algorithm. \n",
    "\n",
    "Hyperparameters:\n",
    "- min_freq: choosing the top N most frequent words - for easier training, weed out words that are irrelevant to us \n",
    "- size: embedding size. We use 300 as per Bolukbasi et al.\n",
    "- window: the window used for looking at context and center words when traing the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "min_freq = 50\n",
    "size = 300\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gutenburg Genism Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "cHtqmychpDEx",
    "outputId": "866140be-8705-4864-b7a7-7598b1f38101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'model_g' (Word2Vec)\n"
     ]
    }
   ],
   "source": [
    "model_g = Word2Vec(min_count=min_freq, vector_size=size, window=window, sg=1) #sg=1 is the skip-gram training algorithm\n",
    "model_g.init_weights()\n",
    "model_g.build_vocab(sentences_gutenburg) \n",
    "\n",
    "#store in Jupiter Notebook to reuse via (%store -r model_g)\n",
    "%store model_g\n",
    "\n",
    "with open('model_g', 'wb') as files:\n",
    "    pickle.dump(model_g, files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wikipedia Genism Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w = Word2Vec(min_count=min_freq, vector_size=size, window=window, sg=1) #sg=1 is the skip-gram training algorithm\n",
    "model_w.init_weights()\n",
    "model_w.build_vocab(sentences_wikipedia) \n",
    "\n",
    "#store in Jupiter Notebook to reuse via (%store -r model_w)\n",
    "%store model_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYDVA5_wpDEy"
   },
   "source": [
    "Now, save the embeddings to use in debiasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "e_XcKc4SpDEy"
   },
   "outputs": [],
   "source": [
    "with open('embeddings/gutenburg_embeddings.txt', 'w') as f:\n",
    "    for idx, key in enumerate(list(model_g.wv.index_to_key)):\n",
    "        embedding = ' '.join(str(v) for v in model_g.wv.get_vector(key))\n",
    "        f.write(key + \" \" + embedding+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9hErZ1ApDEz"
   },
   "outputs": [],
   "source": [
    "with open('embeddings/wikipedia_embeddings.txt', 'w') as f:\n",
    "    for idx, key in enumerate(list(model_w.wv.index_to_key)):\n",
    "        embedding = ' '.join(str(v) for v in model_w.wv.get_vector(key))\n",
    "        f.write(key + \" \" + embedding+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VzYjT6CqAJv"
   },
   "source": [
    "## 3 - Custom Word2Vec \n",
    "\n",
    "Using the same hyperparameters as Genism, train our own word2vec model (withought debiasing) on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "wNHWm-S7qSiD",
    "outputId": "765dcb4b-60da-479a-db8a-b72f7493fff0"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "embedding_dim=300\n",
    "LR=0.01\n",
    "window_size=2\n",
    "EPOCHS=5\n",
    "min_freq = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gutenburg Custom Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Skip Grams...\n",
      "...(0.7419min)\n",
      "Training. Curr Time = 09:04:11 , Estimated Finish Time = 18:09:22\n",
      "Epoch: 1, Training Loss: 6551374.260236576  (78.5356min)\n",
      "Epoch: 2, Training Loss: 6447569.940075085  (78.2289min)\n",
      "Epoch: 3, Training Loss: 6435655.812316194  (78.0902min)\n",
      "Epoch: 4, Training Loss: 6432436.643278345  (78.0330min)\n"
     ]
    }
   ],
   "source": [
    "model_g_custom = Custom_Word2Vec(sentences_gutenburg, embedding_dim, LR, window_size, EPOCHS, min_freq)\n",
    "model_g_custom.train()\n",
    "\n",
    "#store in Jupiter Notebook to reuse via (%store -r model_g_custom)\n",
    "%store model_g_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wikipedia Custom Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_custom = Custom_Word2Vec(sentences_wikipedia, embedding_dim, LR, window_size, EPOCHS, min_freq)\n",
    "model_w_custom.train()\n",
    "\n",
    "#store in Jupiter Notebook to reuse via (%store -r model_w_custom)\n",
    "%store model_w_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, save the embeddings to use in debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/gutenburg_custom_embeddings.txt', 'w') as f:\n",
    "    for word in model_g_custom.corpus_vocab:\n",
    "        embedding = ' '.join(model_g_custom.embedding(word))\n",
    "        f.write(word + \" \" + embedding +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/wikipedia_custom_embeddings.txt', 'w') as f:\n",
    "    for word in model_w_custom.corpus_vocab:\n",
    "        embedding = ' '.join(model_w_custom.embedding(word))\n",
    "        f.write(word + \" \" + embedding +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9jws4VUXiuh"
   },
   "source": [
    "## 4 - Measuring Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 - Define Helper functions\n",
    "\n",
    "Define some useful helper functions to use in the measurement techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(model, model_type):\n",
    "    '''\n",
    "    returns the vocabulary of the model\n",
    "    '''\n",
    "    if model_type == 'genism':\n",
    "        return model.wv.key_to_index\n",
    "    elif model_type == 'custom':\n",
    "        return model.corpus_vocab\n",
    "    else:\n",
    "        print(\"Model type is wrong\")\n",
    "        return []\n",
    "\n",
    "def check_w_embedding(word_lists, model, printRemoved, model_type):\n",
    "    '''\n",
    "    checks if the embedding exists for the given word, and if not deletes it from the lists. \n",
    "    if printRemoved = true, it also declares the words that were not in the model embeddings. \n",
    "    '''\n",
    "    new_w_lists = []\n",
    "    \n",
    "    for word_list in word_lists:\n",
    "        new_w_l = []\n",
    "        for w in word_list: \n",
    "            if w in get_vocab(model, model_type):\n",
    "                new_w_l.append(w)\n",
    "            else:\n",
    "                if printRemoved:\n",
    "                    print (\"Word\", w, \" is not in the model\")\n",
    "                    \n",
    "        new_w_lists.append(new_w_l)\n",
    "    \n",
    "    return new_w_lists\n",
    "\n",
    "def w_vec(word, model, model_type):\n",
    "    '''\n",
    "    returns the word embedding of the input word\n",
    "    '''\n",
    "    if model_type == 'genism':\n",
    "        return model.wv[word]\n",
    "    elif model_type == 'custom':\n",
    "        return model.embedding(word)\n",
    "    else:\n",
    "        print(\"Model type is wrong\")\n",
    "        return []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    '''\n",
    "    returns the cosine similarity between 2 word embeddings \n",
    "    '''\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXVe1S3hpDEz"
   },
   "source": [
    "### 4.1 - Construct the g subspace\n",
    "First, define gendered words and pairs ground truth (taken from Bolukbasi et al.) Using PCA, define the gendered subspace g, by taking the most prominent dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGwiksMipDE0",
    "outputId": "ed4a1e49-169d-4e8a-d123-17f2774b239d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA variance ratio: [3.7489527e-01 2.3151179e-01 2.0511854e-01 1.8847439e-01 8.2512580e-15\n",
      " 2.9779241e-16]\n",
      "PCA singular values: [6.2063221e-02 4.8771475e-02 4.5907304e-02 4.4005353e-02 9.2074508e-09\n",
      " 1.7491866e-09]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gender_subspace_matrix(model, gender_pairs, model_type):\n",
    "    '''\n",
    "    create matrix of the gender space using gender pairs\n",
    "        \n",
    "    '''\n",
    "    word_directions = []\n",
    "    for word_pair in gender_pairs:\n",
    "        f_word = w_vec(word_pair[0], model, model_type)\n",
    "        m_word = w_vec(word_pair[1], model, model_type)\n",
    "        dif = f_word-m_word\n",
    "        word_directions.append(dif)\n",
    "\n",
    "    return np.array(word_directions)\n",
    "\n",
    "\n",
    "def define_vector_g(model, model_type, gender_pairs):\n",
    "    word_directions = gender_subspace_matrix(model, gender_pairs, model_type) \n",
    "\n",
    "    #PCA of word direction\n",
    "    pca = PCA(n_components=len(gender_pairs))\n",
    "    pca.fit(word_directions)\n",
    "\n",
    "    #print graph of PCA dimensions to determine how many dimensions should be kept\n",
    "    print(\"PCA variance ratio:\", pca.explained_variance_ratio_)\n",
    "    print(\"PCA singular values:\", pca.singular_values_)\n",
    "\n",
    "    #plot graph of eigenvalue variance (basically how relevant each dimension was)\n",
    "    len_var = len(pca.explained_variance_ratio_)\n",
    "    plt.pyplot.bar(range(1,len_var+1),pca.explained_variance_ratio_)\n",
    "    plt.pyplot.xlabel(\"Matrix Variance Column\")\n",
    "    plt.pyplot.ylabel(\"Variance\")\n",
    "    plt.pyplot.title(\"Significance of each eigenvector\")\n",
    "\n",
    "    #use the 1 most prominent dimension\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(word_directions)\n",
    "    g = pca.components_\n",
    "    \n",
    "    return g\n",
    "\n",
    "#gender pairs\n",
    "gender_pairs = [['she','he'],['her','his'],['woman','man'],['Mary','John'],['herself','himself'],['daughter','son'],['mother','father'],['gal','guy'],['girl','boy'],['female','male']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender Subspace for Gutenburg Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models from storage\n",
    "%store -r model_g\n",
    "%store -r model_g_custom\n",
    "\n",
    "#genism model\n",
    "print (\"---------Genism-------------\")\n",
    "gender_pairs_g = check_w_embedding(gender_pairs, model_g, True, 'genism')\n",
    "print (\"Gender Pairs Genism Word2Vec: \", gender_pairs_g)\n",
    "g_gutenburg = define_vector_g(model_g, 'genism', gender_pairs_g)\n",
    "\n",
    "print (\"---------Custom-------------\")\n",
    "\n",
    "#custom model\n",
    "gender_pairs_g_c = check_w_embedding(gender_pairs, model_g_custom, True, 'custom')\n",
    "print (\"Gender Pairs Custom Word2Vec: \", gender_pairs_g_c)\n",
    "g_gutenburg_custom = define_vector_g(model_g_custom, 'custom', gender_pairs_g_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender Subspace for Wikipedia Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models from storage\n",
    "%store -r model_w\n",
    "%store -r model_w_custom\n",
    "\n",
    "#genism model\n",
    "print (\"---------Genism-------------\")\n",
    "gender_pairs_w = check_w_embedding(gender_pairs, model_w, True, 'genism')\n",
    "print (\"Gender Pairs Genism Word2Vec: \", gender_pairs_w)\n",
    "g_wiki = define_vector_g(model_w, 'genism', gender_pairs_w)\n",
    "\n",
    "print (\"---------Custom-------------\")\n",
    "\n",
    "#custom model\n",
    "gender_pairs_w_c = check_w_embedding(gender_pairs, model_w_custom, True, 'custom')\n",
    "print (\"Gender Pairs Custom Word2Vec: \", gender_pairs_w_c)\n",
    "g_wiki_custom = define_vector_g(model_w_custom, 'custom', gender_pairs_w_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ylrwq0MXxYN"
   },
   "source": [
    "### 4.2 - Direct Bias\n",
    "\n",
    "To measure direct bias, we will be using the equation from Bolukbasi et al. shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI6_1ww2pDE1"
   },
   "source": [
    "<img src=\"img/Direct_Bias.png\" style=\"width:300px;float:left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBrC38VkpDE1"
   },
   "source": [
    "We will use the same gender neutral words that have been collected from the paper which is \n",
    "denoted by N. As well, we will utilize the paper’s gender directions (vector from one word to another) \n",
    "which were verified by crowdsourcing. The gender subspace g will be the unit vector g that captures the\n",
    "gender directions, computed by principal components in the paper. Finally, we will consider c, which is the \n",
    "strictness of bias, as a hyperparameter in our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc-4WChgpDE1",
    "outputId": "cced117f-befb-4e48-d27c-542cd3e53a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gendered word set after filtering:  ['he', 'his', 'her', 'she', 'him', 'man', 'women', 'men', 'wife', 'himself', 'son', 'mother', 'father', 'daughter', 'husband', 'girls', 'boy', 'boys', 'brother', 'female', 'sister', 'male', 'brothers', 'actress', 'sons', 'lady', 'king', 'ladies', 'uncle', 'bull', 'queen', 'wives', 'fathers', 'maiden', 'gentleman', 'fraternity', 'prince', 'gentlemen', 'lad', 'kings', 'maternal', 'womb', 'countrymen', 'brethren', 'brotherhood', 'statesman', 'goddess', 'baritone', 'queens', 'beau', 'pa']\n",
      "Direct metric gutenburg: 0.09315754678134587\n"
     ]
    }
   ],
   "source": [
    "def calculate_direct_metric(gendered_word_set, model, c, g, model_type):\n",
    "    '''\n",
    "    calculate the direct bias using Bolukbasi et al.'s equation\n",
    "    '''\n",
    "    cosine_add = 0\n",
    "    for word in gendered_word_set:\n",
    "        cosine_add += np.abs(cos_sim(w_vec(word, model, model_type),g[0]))**c\n",
    "    \n",
    "    return cosine_add/len(gendered_word_set)\n",
    "    \n",
    "\n",
    "c = 1 #hyperparameter\n",
    "\n",
    "# Gendered word from Boulbalski et al. \n",
    "gendered_words = 'he, his, her, she, him, man, women, men, woman, spokesman, wife, himself, son, mother, father, chairman, daughter, husband, guy, girls, girl, boy, boys, brother, spokeswoman, female, sister, male, herself, brothers, dad, actress, mom, sons, girlfriend, daughters, lady, boyfriend, sisters, mothers, king, businessman, grandmother, grandfather, deer, ladies, uncle, males, congressman, grandson, bull, queen, businessmen, wives, widow, nephew, bride, females, aunt, prostate cancer, lesbian, chairwoman, fathers, moms, maiden, granddaughter, younger brother, lads, lion, gentleman, fraternity, bachelor, niece, bulls, husbands, prince, colt, salesman, hers, dude, beard, filly, princess, lesbians, councilman, actresses, gentlemen, stepfather, monks, ex girlfriend, lad, sperm, testosterone, nephews, maid, daddy, mare, fiance, fiancee, kings, dads, waitress, maternal, heroine, nieces, girlfriends, sir, stud, mistress, lions, estranged wife, womb, grandma, maternity, estrogen, ex boyfriend, widows, gelding, diva, teenage girls, nuns, czar, ovarian cancer, countrymen, teenage girl, penis, bloke, nun, brides, housewife, spokesmen, suitors, menopause, monastery, motherhood, brethren, stepmother, prostate, hostess, twin brother, schoolboy, brotherhood, fillies, stepson, congresswoman, uncles, witch, monk, viagra, paternity, suitor, sorority, macho, businesswoman, eldest son, gal, statesman, schoolgirl, fathered, goddess, hubby, stepdaughter, blokes, dudes, strongman, uterus, grandsons, studs, mama, godfather, hens, hen, mommy, estranged husband, elder brother, boyhood, baritone, grandmothers, grandpa, boyfriends, feminism, countryman, stallion, heiress, queens, witches, aunts, semen, fella, granddaughters, chap, widower, salesmen, convent, vagina, beau, beards, handyman, twin sister, maids, gals, housewives, horsemen, obstetrics, fatherhood, councilwoman, princes, matriarch, colts, ma, fraternities, pa, fellas, councilmen, dowry, barbershop, fraternal, ballerina'\n",
    "gendered_words = gendered_words.split(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Direct Metric for Gutenburg Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models from storage\n",
    "%store -r model_g\n",
    "%store -r model_g_custom\n",
    "\n",
    "#genism\n",
    "print (\"---------Genism-------------\")\n",
    "gendered_word_set = check_w_embedding([gendered_words], model_g, False, 'genism')[0]\n",
    "print(\"Gendered word set after filtering: \", gendered_word_set)\n",
    "direct_gutenburg = calculate_direct_metric(gendered_word_set, model_g, c, g_gutenburg)\n",
    "print(\"Direct metric gutenburg:\", direct_gutenburg)\n",
    "\n",
    "print (\"---------Custom-------------\")\n",
    "#custom\n",
    "gendered_word_set_c = check_w_embedding([gendered_words], model_g_custom, False, 'custom')[0]\n",
    "print(\"Gendered word set after filtering: \", gendered_word_set_c)\n",
    "direct_gutenburg_custom = calculate_direct_metric(gendered_word_set_c, model_g_custom, c, g_gutenburg_custom)\n",
    "print(\"Direct metric gutenburg:\", direct_gutenburg_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Direct Metric for Wikipedia Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMIUMyqsX57J"
   },
   "source": [
    "### 4.3 - Indirect Bias\n",
    "\n",
    "To measure indirect bias, we will be using the equation from Bolukbasi et al. shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrOSjUQspDE2"
   },
   "source": [
    "<img src=\"img/Indirect_Bias.png\" style=\"width:300px;float:left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6krHTKxepDE2"
   },
   "source": [
    "$w_{\\perp}$ is defined as $w_{\\perp} = w - w_{g}$ where $w_{g}=(w*g)*g$ as mentioned in the paper. We will be using the g subspace calculated above. As for the word pairs, we will run a few experiments. The first will be the most extreme words in the softball-football direction as mentioned in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGyyr-NqpDE2",
    "outputId": "48e36e83-23b3-492e-9a74-eeac5e2bbe8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indirect β values for word 'clean': -938.91296, -926.94385, -775.27264, -869.30945, -910.7613, \n",
      "indirect β values for word 'manager': -898.9482, -933.16833, -884.65454, -905.6266, -903.878, \n"
     ]
    }
   ],
   "source": [
    "def calculate_indirect_metric(g, w, v):\n",
    "    wg = np.dot(np.dot(w,g[0]),g[0])\n",
    "    vg = np.dot(np.dot(v,g[0]),g[0])\n",
    "    w_norm_vec = w-wg\n",
    "    v_norm_vec = v-vg\n",
    "    w_norm = norm(w_norm_vec)\n",
    "    v_norm = norm(v_norm_vec)\n",
    "    \n",
    "    return (np.dot(w,v) - np.dot(w_norm_vec,v_norm_vec)/(w_norm*v_norm))/np.dot(w,v)\n",
    "\n",
    "def compare_indirect_metrics(words, c_word, model,g):\n",
    "    metrics=\"\"\n",
    "    for w in words:\n",
    "        metrics+=str(calculate_indirect_metric(g, w_vec(c_word,model), w_vec(w,model)))+\", \"\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "#Future Experiment - Bolukbasi et al. \n",
    "#extreme_words_w = ['pitcher','bookkeeper', 'receptionist', 'nurse', 'waitress']\n",
    "#compare_word_w = 'softball' - doesn't exist in gutenburg\n",
    "#extreme_words_m = ['footballer','businessman', 'pundit', 'maestro', 'cleric']\n",
    "#compare_word_m = 'football'\n",
    "\n",
    "#Experiment 1\n",
    "extreme_words_w = ['family', 'sister', 'marriage', 'charm', 'relatives']\n",
    "compare_word_w = 'clean'\n",
    "extreme_words_m = ['paper', 'money', 'office', 'business', 'meeting']\n",
    "compare_word_m = 'manager'\n",
    "\n",
    "extreme_words_w,[compare_word_w] = check_w_embedding([extreme_words_w,[compare_word_w]], model_g, True)\n",
    "metrics_w = compare_indirect_metrics(extreme_words_w, compare_word_w, model_g,g_gutenburg)\n",
    "print(\"indirect β values for word 'clean':\", metrics_w)\n",
    "\n",
    "extreme_words_m,[compare_word_w] = check_w_embedding([extreme_words_m,[compare_word_m]], model_g, True)\n",
    "metrics_m = compare_indirect_metrics(extreme_words_m, compare_word_m, model_g,g_gutenburg)\n",
    "print(\"indirect β values for word 'manager':\", metrics_m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOv0RFFOX9NP"
   },
   "source": [
    "### 4.4 - WEAT Metric\n",
    "\n",
    "To measure WEAT, we used the formula defined in Caliskan et al., which is an additional direct bias measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfIcsZSepDE3"
   },
   "source": [
    "<img src=\"img/WEAT.png\" style=\"width:300px;float:left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNYpB3AopDE3"
   },
   "outputs": [],
   "source": [
    "def s_word_A_B(w,A,B,model):\n",
    "    mean_A_sum = 0\n",
    "    mean_B_sum = 0\n",
    "    w_v = w_vec(w,model)\n",
    "    \n",
    "    for a in A:\n",
    "        a_vec = w_vec(a,model)\n",
    "        mean_A_sum +=cos_sim(w_v,a_vec)\n",
    "    \n",
    "    for b in B:\n",
    "        b_vec = w_vec(b,model)\n",
    "        mean_B_sum +=cos_sim(w_v,b_vec)\n",
    "        \n",
    "    return mean_A_sum/len(A) - mean_B_sum/len(B)\n",
    "\n",
    "def s_X_Y_A_B(X,Y,A,B,model):\n",
    "    sum_X = 0\n",
    "    sum_Y = 0\n",
    "    \n",
    "    for x in X:\n",
    "        sum_X += s_word_A_B(x,A,B,model)\n",
    "        \n",
    "    for y in Y:\n",
    "        sum_Y += s_word_A_B(y,A,B,model)\n",
    "    \n",
    "    return sum_X - sum_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e16kg8Z6pDE3"
   },
   "source": [
    "Now, lets run experiments on the models <br>\n",
    "A and B are the attribute word groups (8 words each), and X,Y are the target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3OwU4I6pDE3",
    "outputId": "a7c7a064-6be0-4af6-ef77-d450677946e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAT Exp 1: -0.006268933910178021\n",
      "WEAT Exp 2: -0.0604922525017173\n"
     ]
    }
   ],
   "source": [
    "#Experiment 1 - work/family\n",
    "A = ['tie', 'manager', 'work', 'paper', 'money', 'office', 'business', 'meeting']\n",
    "B = ['home', 'parents', 'children', 'family', 'sister', 'marriage', 'charm', 'relatives']\n",
    "X = ['he', 'him']\n",
    "Y = ['she', 'her'] \n",
    "\n",
    "A,B,X,Y = check_w_embedding([A,B,X,Y], model_g, True)\n",
    "metric_exp_1_g = s_X_Y_A_B(X,Y,A,B,model_g)\n",
    "\n",
    "#A,B,X,Y = check_w_embedding([A,B,X,Y], model_w, True)\n",
    "#metric_exp_1_w = s_X_Y_A_B(X,Y,A,B,model_w)\n",
    "\n",
    "print (\"WEAT Exp 1:\", metric_exp_1_g)\n",
    "\n",
    "#Experiment 2 - professions\n",
    "A_2 = ['police', 'manager', 'business', 'lawyers', 'engineer', 'program', 'builder', 'fire']\n",
    "B_2 = ['teachers', 'clean', 'social', 'hair', 'nail', 'make', 'writer', 'library'] \n",
    "\n",
    "A_3,B_3,X,Y = check_w_embedding([A_2,B_2,X,Y], model_g, True)\n",
    "metric_exp_2_g = s_X_Y_A_B(X,Y,A_2,B_2,model_g)\n",
    "\n",
    "#A_3,B_3,X,Y = check_w_embedding([A_3,B_3,X,Y], model_w, True)\n",
    "#metric_exp_2_w = s_X_Y_A_B(X,Y,A_3,B_3,model_w)\n",
    "\n",
    "print (\"WEAT Exp 2:\", metric_exp_3_g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yRdeOsV4pDE4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec Model and Bias Measurements.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
